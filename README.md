# Sign Language Classification App using MediaPipe Hands and Machine Learning

This project is a demonstration application for the use of MediaPipe Hands and Machine Learning for sign language classification. The application uses the webcam of your computer to detect and track hand gestures in real-time and classify them using a pre-trained machine learning model.

## Requirements

To run this application, you will need to have the following installed:

- Python 3.x
- The libraries listed in the requirements.txt file
To install the necessary libraries, you can use the following command in the command line:

```bash
pip install -r requirements.txt
```

## Usage

Before using this application, it is important to note that its use is intended only for research purposes and is tied to the SEMILL-IAS research seedbed at Universidad del Rosario. Its use for commercial or production applications is not recommended.

Please note that the pre-trained machine learning models required for sign language classification are not available in this repository.

To start the application, run the following command in the command line:

```python
uvicorn app:app

```

## Contributing

If you wish to contribute to this project, you are welcome to fork it and make a pull request with your changes. You can also report any issues or errors you find using Github's "issues" feature.
## License
This project is licensed under the MIT License. Please see the [MIT](https://choosealicense.com/licenses/mit/) file for more information.
